{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-31T15:52:13.938769Z","iopub.execute_input":"2022-01-31T15:52:13.939452Z","iopub.status.idle":"2022-01-31T15:52:13.976581Z","shell.execute_reply.started":"2022-01-31T15:52:13.939361Z","shell.execute_reply":"2022-01-31T15:52:13.975936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom random import seed\nfrom random import randint\nimport random\nimport string\nimport re\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:52:13.978181Z","iopub.execute_input":"2022-01-31T15:52:13.978654Z","iopub.status.idle":"2022-01-31T15:52:13.983611Z","shell.execute_reply.started":"2022-01-31T15:52:13.978617Z","shell.execute_reply":"2022-01-31T15:52:13.982618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport transformers\n\nimport nltk\n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nplt.style.use('seaborn')","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:52:13.98563Z","iopub.execute_input":"2022-01-31T15:52:13.986282Z","iopub.status.idle":"2022-01-31T15:52:17.048163Z","shell.execute_reply.started":"2022-01-31T15:52:13.986243Z","shell.execute_reply":"2022-01-31T15:52:17.047427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download(\"stopwords\")\nnltk.download(\"words\")\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import words","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:52:17.050096Z","iopub.execute_input":"2022-01-31T15:52:17.050511Z","iopub.status.idle":"2022-01-31T15:52:17.258577Z","shell.execute_reply.started":"2022-01-31T15:52:17.050474Z","shell.execute_reply":"2022-01-31T15:52:17.257748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, Reshape, Conv1D, Conv2D, BatchNormalization, MaxPooling1D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, Sequential\n# from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:52:17.259919Z","iopub.execute_input":"2022-01-31T15:52:17.260327Z","iopub.status.idle":"2022-01-31T15:52:22.524005Z","shell.execute_reply.started":"2022-01-31T15:52:17.260288Z","shell.execute_reply":"2022-01-31T15:52:22.523238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed(1)\nstop_words = stopwords.words('english')\nwords=words.words()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:52:22.526003Z","iopub.execute_input":"2022-01-31T15:52:22.526267Z","iopub.status.idle":"2022-01-31T15:52:22.613148Z","shell.execute_reply.started":"2022-01-31T15:52:22.526231Z","shell.execute_reply":"2022-01-31T15:52:22.612422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:52:22.614505Z","iopub.execute_input":"2022-01-31T15:52:22.614826Z","iopub.status.idle":"2022-01-31T15:52:22.668542Z","shell.execute_reply.started":"2022-01-31T15:52:22.614787Z","shell.execute_reply":"2022-01-31T15:52:22.66782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH_CSV_TRAIN = '../input/openofficepreprocessed6/openoffice_preprocessed.csv'\n# PATH_CSV_TEST = '../input/nlpgettingstarted/test.csv'\n# PATH_CSV_SUBMISSION = '../input/nlpgettingstarted/sample_submission.csv'\n\ndataset = pd.read_csv(PATH_CSV_TRAIN)\n# dataf_test = pd.read_csv(PATH_CSV_TEST)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:52:22.670129Z","iopub.execute_input":"2022-01-31T15:52:22.670647Z","iopub.status.idle":"2022-01-31T15:52:24.393206Z","shell.execute_reply.started":"2022-01-31T15:52:22.67061Z","shell.execute_reply":"2022-01-31T15:52:24.392485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.dropna(subset = [\"description2\"], inplace=True)\ndataset.dropna(subset = [\"short_desc2\"], inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reset_column_names():\n  dataset.drop('description1', axis=1, inplace=True)\n  dataset.drop('short_desc1', axis=1, inplace=True)\n  dataset.drop('description2', axis=1, inplace=True)\n  dataset.drop('short_desc2', axis=1, inplace=True)\n\n  dataset.rename(columns={'description1_clean':'description1','short_desc1_clean':'short_desc1','description2_clean':'description2','short_desc2_clean':'short_desc2'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:59:54.727869Z","iopub.execute_input":"2022-01-31T15:59:54.72833Z","iopub.status.idle":"2022-01-31T15:59:54.735221Z","shell.execute_reply.started":"2022-01-31T15:59:54.728293Z","shell.execute_reply":"2022-01-31T15:59:54.734498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    clean=text\n    \n    reg = re.compile(r\" +\")\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n\n    #Lowercase\n    clean = clean.apply(lambda r: r.lower())\n    return clean","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:59:54.736566Z","iopub.execute_input":"2022-01-31T15:59:54.737099Z","iopub.status.idle":"2022-01-31T15:59:54.747796Z","shell.execute_reply.started":"2022-01-31T15:59:54.73704Z","shell.execute_reply":"2022-01-31T15:59:54.747101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['description1_clean'] = clean_text(dataset['description1'])\ndataset['short_desc1_clean'] = clean_text(dataset['short_desc1'])\ndataset['description2_clean'] = clean_text(dataset['description2'])\ndataset['short_desc2_clean'] = clean_text(dataset['short_desc2'])\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:59:54.749165Z","iopub.execute_input":"2022-01-31T15:59:54.74966Z","iopub.status.idle":"2022-01-31T16:00:08.95418Z","shell.execute_reply.started":"2022-01-31T15:59:54.749623Z","shell.execute_reply":"2022-01-31T16:00:08.953491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_column_names()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T16:00:08.955356Z","iopub.execute_input":"2022-01-31T16:00:08.956086Z","iopub.status.idle":"2022-01-31T16:00:09.071924Z","shell.execute_reply.started":"2022-01-31T16:00:08.956047Z","shell.execute_reply":"2022-01-31T16:00:09.071096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in dataset.iterrows():\n    desc = row['description1'] + row['description2']\n    dataset.loc[index, 'description'] = desc","metadata":{"execution":{"iopub.status.busy":"2022-01-31T16:00:09.080111Z","iopub.execute_input":"2022-01-31T16:00:09.080527Z","iopub.status.idle":"2022-01-31T16:04:33.509722Z","shell.execute_reply.started":"2022-01-31T16:00:09.080488Z","shell.execute_reply":"2022-01-31T16:04:33.508966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFXLNetModel, XLNetTokenizer, XLNetConfig","metadata":{"execution":{"iopub.status.busy":"2022-01-31T16:04:33.511078Z","iopub.execute_input":"2022-01-31T16:04:33.511326Z","iopub.status.idle":"2022-01-31T16:04:33.678332Z","shell.execute_reply.started":"2022-01-31T16:04:33.511293Z","shell.execute_reply":"2022-01-31T16:04:33.677623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xlnet_model = 'xlnet-base-cased'\nxlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T16:04:33.679423Z","iopub.execute_input":"2022-01-31T16:04:33.679656Z","iopub.status.idle":"2022-01-31T16:04:37.96601Z","shell.execute_reply.started":"2022-01-31T16:04:33.679622Z","shell.execute_reply":"2022-01-31T16:04:37.965296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_xlnet(mname):\n    \"\"\" Creates the model. It is composed of the XLNet main block and then\n    a classification head its added\n    \"\"\"\n    # Define token ids as inputs\n    word_inputs = tf.keras.Input(shape=(120,), name='word_inputs', dtype='int32')\n\n    # Call XLNet model\n    xlnet = TFXLNetModel.from_pretrained(mname)\n    xlnet_encodings = xlnet(word_inputs)[0]\n\n    # CLASSIFICATION HEAD \n    # Collect last step from last hidden state (CLS)\n    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n    print(doc_encoding)\n    # Apply dropout for regularization\n    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n    # Final output \n    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(doc_encoding)\n\n    # Compile model\n    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-31T16:04:37.967415Z","iopub.execute_input":"2022-01-31T16:04:37.967667Z","iopub.status.idle":"2022-01-31T16:04:37.976028Z","shell.execute_reply.started":"2022-01-31T16:04:37.967633Z","shell.execute_reply":"2022-01-31T16:04:37.975262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_inputs(text, tokenizer, max_len=120):\n    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n#     text = [w.lower() for w in text]\n#     text = [w for w in text if w.lower() not in stop_words]\n#     text = [w for w in text if w in words or w.isalpha()]\n    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in text]\n    inp_tok = np.array([a['input_ids'] for a in inps])\n    ids = np.array([a['attention_mask'] for a in inps])\n    segments = np.array([a['token_type_ids'] for a in inps])\n    return inp_tok, ids, segments","metadata":{"execution":{"iopub.status.busy":"2022-01-31T16:04:37.977501Z","iopub.execute_input":"2022-01-31T16:04:37.977806Z","iopub.status.idle":"2022-01-31T16:04:37.988566Z","shell.execute_reply.started":"2022-01-31T16:04:37.977769Z","shell.execute_reply":"2022-01-31T16:04:37.987936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dataset['description']\nY = np.array(dataset.duplicate.values, dtype='int')\n\nkfold = StratifiedKFold()\ncvscores = []\n    \nfor train, test in kfold.split(X, Y):\n\n    # Prepare data\n    inp_tok, ids, segments = get_inputs(X.iloc[train], xlnet_tokenizer)\n    y_train = np.array(Y[train], dtype='int')\n    \n    xlnet = create_xlnet(xlnet_model)\n    xlnet.summary()\n    # Fit the model  \n    hist = xlnet.fit(x=inp_tok, y=y_train, epochs=5, batch_size=10, validation_split=.15)\n    \n    test_inp_tok, test_ids, test_segments = get_inputs(X.iloc[test], xlnet_tokenizer)\n    y_test = np.array(Y[test], dtype='int')\n\n    # Evaluate the model\n    scores = xlnet.evaluate(test_inp_tok,y_test, verbose=0)\n    f1score = (2 * scores[2]*100 * scores[3]*100)/(scores[2]*100 + scores[3]*100)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n    print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n    print(\"f1 score: \", (f1score))\n    cvscores.append(scores[1] * 100)\n    precision_scores.append(scores[2] * 100)\n    recall_scores.append(scores[3] * 100)\n    f1_scores.append(f1score)\n\nprint(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\nprint(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision_scores), np.std(precision_scores)))\nprint(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall_scores), np.std(recall_scores)))\nprint(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))","metadata":{"execution":{"iopub.status.busy":"2022-01-31T16:05:05.731686Z","iopub.execute_input":"2022-01-31T16:05:05.732194Z"},"trusted":true},"execution_count":null,"outputs":[]}]}