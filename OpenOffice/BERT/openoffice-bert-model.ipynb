{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-07T14:27:25.160015Z","iopub.execute_input":"2022-03-07T14:27:25.160306Z","iopub.status.idle":"2022-03-07T14:27:25.172738Z","shell.execute_reply.started":"2022-03-07T14:27:25.160276Z","shell.execute_reply":"2022-03-07T14:27:25.171936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/tools/tokenization.py","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:25.174943Z","iopub.execute_input":"2022-03-07T14:27:25.175472Z","iopub.status.idle":"2022-03-07T14:27:26.173068Z","shell.execute_reply.started":"2022-03-07T14:27:25.175432Z","shell.execute_reply":"2022-03-07T14:27:26.171978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:26.175398Z","iopub.execute_input":"2022-03-07T14:27:26.176242Z","iopub.status.idle":"2022-03-07T14:27:33.827553Z","shell.execute_reply.started":"2022-03-07T14:27:26.176198Z","shell.execute_reply":"2022-03-07T14:27:33.826631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_hub","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:33.831539Z","iopub.execute_input":"2022-03-07T14:27:33.831813Z","iopub.status.idle":"2022-03-07T14:27:41.687838Z","shell.execute_reply.started":"2022-03-07T14:27:33.831780Z","shell.execute_reply":"2022-03-07T14:27:41.686977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:41.691880Z","iopub.execute_input":"2022-03-07T14:27:41.692145Z","iopub.status.idle":"2022-03-07T14:27:49.219689Z","shell.execute_reply.started":"2022-03-07T14:27:41.692114Z","shell.execute_reply":"2022-03-07T14:27:49.218831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download(\"stopwords\")\nnltk.download(\"words\")\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import words\n# words = set(nltk.corpus.words.words())","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:49.223302Z","iopub.execute_input":"2022-03-07T14:27:49.223534Z","iopub.status.idle":"2022-03-07T14:27:49.356531Z","shell.execute_reply.started":"2022-03-07T14:27:49.223505Z","shell.execute_reply":"2022-03-07T14:27:49.355249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom random import seed\nfrom random import randint\nimport random\nimport string\nimport re\nimport matplotlib.pyplot as plt\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:49.357985Z","iopub.execute_input":"2022-03-07T14:27:49.358277Z","iopub.status.idle":"2022-03-07T14:27:49.363196Z","shell.execute_reply.started":"2022-03-07T14:27:49.358240Z","shell.execute_reply":"2022-03-07T14:27:49.362369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, Reshape, Conv1D, Conv2D, BatchNormalization, MaxPooling1D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, Sequential\n# from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\n# from bert import bert_tokenization\nimport tokenization\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:49.364807Z","iopub.execute_input":"2022-03-07T14:27:49.365090Z","iopub.status.idle":"2022-03-07T14:27:49.373386Z","shell.execute_reply.started":"2022-03-07T14:27:49.365033Z","shell.execute_reply":"2022-03-07T14:27:49.372648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\nsession = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:49.376309Z","iopub.execute_input":"2022-03-07T14:27:49.376494Z","iopub.status.idle":"2022-03-07T14:27:49.389837Z","shell.execute_reply.started":"2022-03-07T14:27:49.376471Z","shell.execute_reply":"2022-03-07T14:27:49.389139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed(1)\nstop_words = stopwords.words('english')\nwords=words.words()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:49.391508Z","iopub.execute_input":"2022-03-07T14:27:49.391960Z","iopub.status.idle":"2022-03-07T14:27:49.480015Z","shell.execute_reply.started":"2022-03-07T14:27:49.391921Z","shell.execute_reply":"2022-03-07T14:27:49.479233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('../input/openofficepreprocessed6/openoffice_preprocessed.csv')\ndataset","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:49.483509Z","iopub.execute_input":"2022-03-07T14:27:49.483715Z","iopub.status.idle":"2022-03-07T14:27:50.831624Z","shell.execute_reply.started":"2022-03-07T14:27:49.483689Z","shell.execute_reply":"2022-03-07T14:27:50.830919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.dropna(subset = [\"description2\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:50.833001Z","iopub.execute_input":"2022-03-07T14:27:50.833290Z","iopub.status.idle":"2022-03-07T14:27:50.867182Z","shell.execute_reply.started":"2022-03-07T14:27:50.833254Z","shell.execute_reply":"2022-03-07T14:27:50.866432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.dropna(subset = [\"short_desc2\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:50.869064Z","iopub.execute_input":"2022-03-07T14:27:50.869466Z","iopub.status.idle":"2022-03-07T14:27:50.899782Z","shell.execute_reply.started":"2022-03-07T14:27:50.869429Z","shell.execute_reply":"2022-03-07T14:27:50.898943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset.loc[dataset['bug_id'] == 15957]","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:50.901174Z","iopub.execute_input":"2022-03-07T14:27:50.901676Z","iopub.status.idle":"2022-03-07T14:27:50.906173Z","shell.execute_reply.started":"2022-03-07T14:27:50.901639Z","shell.execute_reply":"2022-03-07T14:27:50.905224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in dataset.iterrows():\n    doc1 = 'Product:' + row['product'] + '.Component:' + row['component'] + '.Description:' + row['description1'] #+ '.Summary:' + row['short_desc1']\n#     print(row['bug_id'])\n#     print(row['description2'])\n    doc2 = 'Product:' + row['product'] + '.Component:' + row['component'] + '.Description:' + row['description2'] #+ '.Summary:' + row['short_desc2']\n    dataset.loc[index, 'doc1'] = doc1\n    dataset.loc[index, 'doc2'] = doc2","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:27:50.907550Z","iopub.execute_input":"2022-03-07T14:27:50.908285Z","iopub.status.idle":"2022-03-07T14:35:11.974690Z","shell.execute_reply.started":"2022-03-07T14:27:50.908248Z","shell.execute_reply":"2022-03-07T14:35:11.973921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in dataset.iterrows():\n    desc = row['doc1'] + row['doc2']\n    dataset.loc[index, 'description'] = desc","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:35:11.976193Z","iopub.execute_input":"2022-03-07T14:35:11.976444Z","iopub.status.idle":"2022-03-07T14:38:32.666432Z","shell.execute_reply.started":"2022-03-07T14:35:11.976411Z","shell.execute_reply":"2022-03-07T14:38:32.665654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reset_column_names():\n  dataset.drop('description', axis=1, inplace=True)\n  dataset.drop('short_desc1', axis=1, inplace=True)\n  dataset.drop('description2', axis=1, inplace=True)\n  dataset.drop('short_desc2', axis=1, inplace=True)\n\n  dataset.rename(columns={'description_clean':'description'}, inplace=True) #,'short_desc1_clean':'short_desc1','description2_clean':'description2','short_desc2_clean':'short_desc2'","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:32.667723Z","iopub.execute_input":"2022-03-07T14:38:32.667979Z","iopub.status.idle":"2022-03-07T14:38:32.674781Z","shell.execute_reply.started":"2022-03-07T14:38:32.667945Z","shell.execute_reply":"2022-03-07T14:38:32.674118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text_content(text):\n    text = text.lower()\n#     cleaned_text = re.sub(r\"\"\"\n#                [,.;:@#?!&$*+-=_%<>\\/\\[\\]\\(\\)\\{\\}\\\"\\'\\n]+  # Accept one or more copies of punctuation\n#                \\ *           # plus zero or more copies of a space,\n#                \"\"\",\n#                \" \",          # and replace it with a single space\n#                text, flags=re.VERBOSE)\n#     cleaned_text = re.sub(r\"https?:\\/\\/[A-Za-z0-9.\\/?&#+*+-=_%]+\", \" \",text) #replace urls\n    cleaned_text = re.sub(r\" +\", \" \", text) #remove extra white spaces\n#     cleaned_text = re.sub(r\"[d|D]escription\", \"\", cleaned_text)\n    \n    return cleaned_text","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:32.676089Z","iopub.execute_input":"2022-03-07T14:38:32.676485Z","iopub.status.idle":"2022-03-07T14:38:32.684819Z","shell.execute_reply.started":"2022-03-07T14:38:32.676447Z","shell.execute_reply":"2022-03-07T14:38:32.684068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['description_clean']= dataset['description'].apply(lambda x:preprocess_text_content(x))\n# dataset['short_desc1_clean']= dataset['short_desc1'].apply(lambda x:preprocess_text(x))\n# dataset['description2_clean']= dataset['description2'].apply(lambda x:preprocess_text(x))\n# dataset['short_desc2_clean']= dataset['short_desc2'].apply(lambda x:preprocess_text(x))\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:32.687917Z","iopub.execute_input":"2022-03-07T14:38:32.688136Z","iopub.status.idle":"2022-03-07T14:38:37.876937Z","shell.execute_reply.started":"2022-03-07T14:38:32.688111Z","shell.execute_reply":"2022-03-07T14:38:37.876254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_column_names()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:37.878336Z","iopub.execute_input":"2022-03-07T14:38:37.878590Z","iopub.status.idle":"2022-03-07T14:38:38.001842Z","shell.execute_reply.started":"2022-03-07T14:38:37.878554Z","shell.execute_reply":"2022-03-07T14:38:38.001088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def preprocess_text(text,tokenizer):\n# #     print(text)\n#     #tokenize the text\n#     text = tokenizer.tokenize(text)\n#     #lowercase the text\n#     text = [w.lower() for w in text]\n#     #removal of stop word \n#     text = [w for w in text if w not in stop_words]\n#     #removal of non-english words\n#     text = [w for w in text if w in words or w.isalpha()]\n    #stemming\n#     text = [ ps.stem(w) for w in text]\n    #remove extra white spaces\n#     text = [ re.sub(r\" +\", \" \", w) for w in text] \n#     cleaned_text = re.sub(r\"\"\"\n#                [,.;:@#?!&$*+-=_%<>\\/\\[\\]\\(\\)\\{\\}\\\"\\'\\n]+  # Accept one or more copies of punctuation\n#                \\ *           # plus zero or more copies of a space,\n#                \"\"\",\n#                \" \",          # and replace it with a single space\n#                text, flags=re.VERBOSE)\n#     cleaned_text = re.sub(r\"https?:\\/\\/[A-Za-z0-9.\\/?&#+*+-=_%]+\", \" \",cleaned_text) #replace urls\n    \n#     cleaned_text = re.sub(r\"[d|D]escription\", \"\", cleaned_text)\n    \n#     return text","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:38.003096Z","iopub.execute_input":"2022-03-07T14:38:38.003766Z","iopub.status.idle":"2022-03-07T14:38:38.009907Z","shell.execute_reply.started":"2022-03-07T14:38:38.003720Z","shell.execute_reply":"2022-03-07T14:38:38.009249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_len = 0\n# lenArr = []\n# outliers = []\n\n# q25, q75 = np.percentile(lenArr, [25, 75])\n# bin_width = 2 * (q75 - q25) * len(lenArr) ** (-1/3)\n# bins = round((lenArr.max() - lenArr.min()) / bin_width)\n\n### Add tokens to the data make it BERT compatible\ndef bert_encode(texts, tokenizer, max_len=512):\n    print('encode')\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n#         text = preprocess_text(text,tokenizer)\n        text = tokenizer.tokenize(text)\n#         print(text)\n        text = [w for w in text if w.lower() not in stop_words]\n    \n#         length = len(text)\n#         if(length <500):\n#             lenArr.append(length)\n#     #     print(length)\n#             if(max_len < length):\n#               max_len = length\n#         else:\n#             outliers.append(length)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n#     print(len(lenArr))\n#     # lenArr=list(filter(lambda a: a != 3395, lenArr))\n#     # print(lenArr)\n#     print(\"avg\", sum(lenArr)/len(lenArr))\n#     print(\"outliers\", outliers)\n#     print(\"outliers\", len(outliers))\n#     plt.hist(lenArr, density=True, bins=30)\n#     max_len\n\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:38.011280Z","iopub.execute_input":"2022-03-07T14:38:38.011607Z","iopub.status.idle":"2022-03-07T14:38:38.021674Z","shell.execute_reply.started":"2022-03-07T14:38:38.011569Z","shell.execute_reply":"2022-03-07T14:38:38.020769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    print('build model')\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    \n#     embedding = bert_layer(\n#         input_word_ids, token_type_ids=segment_ids, attention_mask=input_mask\n#     )[0]\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :] \n#     print(clf_output)\n   \n#     clf_output = Reshape((1024,1))(clf_output)\n#     print(clf_output)\n    \n#     x = Conv1D(8, 3, activation='relu', padding='same')(clf_output)\n#     x = BatchNormalization()(x)\n#     x = MaxPooling1D()(x)\n#     x = Conv1D(32, 3, activation='relu', padding='same')(x)\n#     x = BatchNormalization()(x)\n#     x = MaxPooling1D()(x)\n#     x = Flatten()(x)\n#     out = Dense(1, activation='sigmoid')(x)\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:38.023026Z","iopub.execute_input":"2022-03-07T14:38:38.023852Z","iopub.status.idle":"2022-03-07T14:38:38.033898Z","shell.execute_reply.started":"2022-03-07T14:38:38.023813Z","shell.execute_reply":"2022-03-07T14:38:38.033212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(dataset, test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:38.035985Z","iopub.execute_input":"2022-03-07T14:38:38.037556Z","iopub.status.idle":"2022-03-07T14:38:38.117439Z","shell.execute_reply.started":"2022-03-07T14:38:38.037517Z","shell.execute_reply":"2022-03-07T14:38:38.116661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FullTokenizer = tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\", trainable=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:38.118554Z","iopub.execute_input":"2022-03-07T14:38:38.118805Z","iopub.status.idle":"2022-03-07T14:38:59.877194Z","shell.execute_reply.started":"2022-03-07T14:38:38.118771Z","shell.execute_reply":"2022-03-07T14:38:59.876409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nlower_cased = bert_layer.resolved_object.do_lower_case.numpy()\n\ntokenizer = FullTokenizer(vocab_file, lower_cased)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:59.878706Z","iopub.execute_input":"2022-03-07T14:38:59.878970Z","iopub.status.idle":"2022-03-07T14:38:59.994204Z","shell.execute_reply.started":"2022-03-07T14:38:59.878932Z","shell.execute_reply":"2022-03-07T14:38:59.993421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input = bert_encode(train['description'], tokenizer, max_len=160)\ntest_input = bert_encode(test['description'], tokenizer, max_len=160)\ntrain_labels = np.array(train.duplicate.values, dtype='int')\ntest_labels = np.array(test.duplicate.values, dtype='int')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:38:59.995458Z","iopub.execute_input":"2022-03-07T14:38:59.995762Z","iopub.status.idle":"2022-03-07T14:48:59.704517Z","shell.execute_reply.started":"2022-03-07T14:38:59.995719Z","shell.execute_reply":"2022-03-07T14:48:59.703785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_len = 0\n# lenArr = []\n# outliers = []\n\n# for index, row in dataset.iterrows():\n#     # print(train.loc[[index]]['description'])\n#     length = max([len(s.split()) for s in dataset.loc[[index]]['description']])\n#     if(length <1000):\n#         lenArr.append(length)\n# #     print(length)\n#         if(max_len < length):\n#           max_len = length\n#     else:\n#         outliers.append(length)\n\n# # q25, q75 = np.percentile(lenArr, [25, 75])\n# # bin_width = 2 * (q75 - q25) * len(lenArr) ** (-1/3)\n# # bins = round((lenArr.max() - lenArr.min()) / bin_width)\n# print(len(lenArr))\n# # lenArr=list(filter(lambda a: a != 3395, lenArr))\n# # print(lenArr)\n# print(\"avg\", sum(lenArr)/len(lenArr))\n# print(\"outliers\", outliers)\n# print(\"outliers\", len(outliers))\n# plt.hist(lenArr, density=True, bins=30)\n# max_len","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:48:59.705882Z","iopub.execute_input":"2022-03-07T14:48:59.706141Z","iopub.status.idle":"2022-03-07T14:48:59.711745Z","shell.execute_reply.started":"2022-03-07T14:48:59.706107Z","shell.execute_reply":"2022-03-07T14:48:59.710976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:48:59.716541Z","iopub.execute_input":"2022-03-07T14:48:59.717090Z","iopub.status.idle":"2022-03-07T14:49:00.869447Z","shell.execute_reply.started":"2022-03-07T14:48:59.717023Z","shell.execute_reply":"2022-03-07T14:49:00.868690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_history = model.fit(\n    train_input, train_labels,\n    epochs=1,\n    batch_size=15\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:49:00.870815Z","iopub.execute_input":"2022-03-07T14:49:00.871026Z","iopub.status.idle":"2022-03-07T14:49:44.305864Z","shell.execute_reply.started":"2022-03-07T14:49:00.870994Z","shell.execute_reply":"2022-03-07T14:49:44.304690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"openoffice-bert-model\")","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:49:44.306777Z","iopub.status.idle":"2022-03-07T14:49:44.307193Z","shell.execute_reply.started":"2022-03-07T14:49:44.306958Z","shell.execute_reply":"2022-03-07T14:49:44.306979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_results = model.evaluate(test_input,test_labels,return_dict=True)\nevaluation_results","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:49:44.309052Z","iopub.status.idle":"2022-03-07T14:49:44.309395Z","shell.execute_reply.started":"2022-03-07T14:49:44.309211Z","shell.execute_reply":"2022-03-07T14:49:44.309230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X = dataset['description']\n# Y = np.array(dataset.duplicate.values, dtype='int')\n\n# kfold = StratifiedKFold()\n# cvscores = []\n# precision_scores = []\n# recall_scores = []\n# f1_scores = []\n    \n# for train, test in kfold.split(X, Y):\n#     # Prepare data\n#     train_input = bert_encode(X.iloc[train], tokenizer, max_len=160)\n#     test_input = bert_encode(X.iloc[test], tokenizer, max_len=160)\n#     train_labels = Y[train]\n#     test_labels = Y[test]\n    \n#     model = build_model(bert_layer, max_len=160)\n#     model.summary()\n\n#     # Fit the model\n#     model.fit(train_input,train_labels, epochs=1, batch_size=15)\n\n#     # Evaluate the model\n#     scores = model.evaluate(test_input, test_labels, verbose=0)\n#     f1score = (2 * scores[2]*100 * scores[3]*100)/(scores[2]*100 + scores[3]*100)\n#     print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n#     print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n#     print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n#     print(\"f1 score: \", (f1score))\n#     cvscores.append(scores[1] * 100)\n#     precision_scores.append(scores[2] * 100)\n#     recall_scores.append(scores[3] * 100)\n#     f1_scores.append(f1score)\n    \n\n# print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n# print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision_scores), np.std(precision_scores)))\n# print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall_scores), np.std(recall_scores)))\n# print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-03-07T14:49:44.310316Z","iopub.status.idle":"2022-03-07T14:49:44.310646Z","shell.execute_reply.started":"2022-03-07T14:49:44.310458Z","shell.execute_reply":"2022-03-07T14:49:44.310475Z"},"trusted":true},"execution_count":null,"outputs":[]}]}