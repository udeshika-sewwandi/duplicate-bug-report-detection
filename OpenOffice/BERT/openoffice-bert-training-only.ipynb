{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-09T08:04:05.931271Z","iopub.execute_input":"2022-02-09T08:04:05.931624Z","iopub.status.idle":"2022-02-09T08:04:05.963168Z","shell.execute_reply.started":"2022-02-09T08:04:05.931537Z","shell.execute_reply":"2022-02-09T08:04:05.962448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/tools/tokenization.py","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:05.965011Z","iopub.execute_input":"2022-02-09T08:04:05.965498Z","iopub.status.idle":"2022-02-09T08:04:06.863095Z","shell.execute_reply.started":"2022-02-09T08:04:05.965461Z","shell.execute_reply":"2022-02-09T08:04:06.861998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:06.865Z","iopub.execute_input":"2022-02-09T08:04:06.865259Z","iopub.status.idle":"2022-02-09T08:04:15.385133Z","shell.execute_reply.started":"2022-02-09T08:04:06.86523Z","shell.execute_reply":"2022-02-09T08:04:15.384326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_hub","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:15.386849Z","iopub.execute_input":"2022-02-09T08:04:15.387111Z","iopub.status.idle":"2022-02-09T08:04:22.867767Z","shell.execute_reply.started":"2022-02-09T08:04:15.387075Z","shell.execute_reply":"2022-02-09T08:04:22.86682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:22.871808Z","iopub.execute_input":"2022-02-09T08:04:22.872071Z","iopub.status.idle":"2022-02-09T08:04:30.844702Z","shell.execute_reply.started":"2022-02-09T08:04:22.87204Z","shell.execute_reply":"2022-02-09T08:04:30.84387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download(\"stopwords\")\nnltk.download(\"words\")\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import words\n# words = set(nltk.corpus.words.words())","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:30.84655Z","iopub.execute_input":"2022-02-09T08:04:30.846838Z","iopub.status.idle":"2022-02-09T08:04:32.288831Z","shell.execute_reply.started":"2022-02-09T08:04:30.846792Z","shell.execute_reply":"2022-02-09T08:04:32.287934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom random import seed\nfrom random import randint\nimport random\nimport string\nimport re\nimport matplotlib.pyplot as plt\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:32.290438Z","iopub.execute_input":"2022-02-09T08:04:32.290921Z","iopub.status.idle":"2022-02-09T08:04:32.295743Z","shell.execute_reply.started":"2022-02-09T08:04:32.290882Z","shell.execute_reply":"2022-02-09T08:04:32.294937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, Reshape, Conv1D, Conv2D, BatchNormalization, MaxPooling1D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, Sequential\n# from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\n# from bert import bert_tokenization\nimport tokenization\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:32.297101Z","iopub.execute_input":"2022-02-09T08:04:32.297371Z","iopub.status.idle":"2022-02-09T08:04:37.178915Z","shell.execute_reply.started":"2022-02-09T08:04:32.297332Z","shell.execute_reply":"2022-02-09T08:04:37.178174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\nsession = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:37.180121Z","iopub.execute_input":"2022-02-09T08:04:37.18041Z","iopub.status.idle":"2022-02-09T08:04:39.108912Z","shell.execute_reply.started":"2022-02-09T08:04:37.18034Z","shell.execute_reply":"2022-02-09T08:04:39.107909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed(1)\nstop_words = stopwords.words('english')\nwords=words.words()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:39.110004Z","iopub.execute_input":"2022-02-09T08:04:39.110239Z","iopub.status.idle":"2022-02-09T08:04:39.199503Z","shell.execute_reply.started":"2022-02-09T08:04:39.110209Z","shell.execute_reply":"2022-02-09T08:04:39.198728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('../input/openofficepreprocessed6/openoffice_preprocessed.csv')\ndataset","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:39.200763Z","iopub.execute_input":"2022-02-09T08:04:39.201021Z","iopub.status.idle":"2022-02-09T08:04:42.00415Z","shell.execute_reply.started":"2022-02-09T08:04:39.200987Z","shell.execute_reply":"2022-02-09T08:04:42.003365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.dropna(subset = [\"description2\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:42.00542Z","iopub.execute_input":"2022-02-09T08:04:42.005752Z","iopub.status.idle":"2022-02-09T08:04:42.043646Z","shell.execute_reply.started":"2022-02-09T08:04:42.005708Z","shell.execute_reply":"2022-02-09T08:04:42.042942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.dropna(subset = [\"short_desc2\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:42.044762Z","iopub.execute_input":"2022-02-09T08:04:42.045Z","iopub.status.idle":"2022-02-09T08:04:42.078225Z","shell.execute_reply.started":"2022-02-09T08:04:42.044966Z","shell.execute_reply":"2022-02-09T08:04:42.077552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset.loc[dataset['bug_id'] == 15957]","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:42.081546Z","iopub.execute_input":"2022-02-09T08:04:42.08192Z","iopub.status.idle":"2022-02-09T08:04:42.085313Z","shell.execute_reply.started":"2022-02-09T08:04:42.081889Z","shell.execute_reply":"2022-02-09T08:04:42.084517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in dataset.iterrows():\n    doc1 = 'Product:' + row['product'] + '.Component:' + row['component'] + '.Description:' + row['description1'] #+ '.Summary:' + row['short_desc1']\n#     print(row['bug_id'])\n#     print(row['description2'])\n    doc2 = 'Product:' + row['product'] + '.Component:' + row['component'] + '.Description:' + row['description2'] #+ '.Summary:' + row['short_desc2']\n    dataset.loc[index, 'doc1'] = doc1\n    dataset.loc[index, 'doc2'] = doc2","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:04:42.086687Z","iopub.execute_input":"2022-02-09T08:04:42.087102Z","iopub.status.idle":"2022-02-09T08:11:38.509755Z","shell.execute_reply.started":"2022-02-09T08:04:42.087006Z","shell.execute_reply":"2022-02-09T08:11:38.508909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in dataset.iterrows():\n    desc = row['doc1'] + row['doc2']\n    dataset.loc[index, 'description'] = desc","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:11:38.514555Z","iopub.execute_input":"2022-02-09T08:11:38.516556Z","iopub.status.idle":"2022-02-09T08:14:56.839927Z","shell.execute_reply.started":"2022-02-09T08:11:38.516516Z","shell.execute_reply":"2022-02-09T08:14:56.839152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reset_column_names():\n  dataset.drop('description', axis=1, inplace=True)\n  dataset.drop('short_desc1', axis=1, inplace=True)\n  dataset.drop('description2', axis=1, inplace=True)\n  dataset.drop('short_desc2', axis=1, inplace=True)\n\n  dataset.rename(columns={'description_clean':'description'}, inplace=True) #,'short_desc1_clean':'short_desc1','description2_clean':'description2','short_desc2_clean':'short_desc2'","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:14:56.84125Z","iopub.execute_input":"2022-02-09T08:14:56.841511Z","iopub.status.idle":"2022-02-09T08:14:56.848248Z","shell.execute_reply.started":"2022-02-09T08:14:56.841477Z","shell.execute_reply":"2022-02-09T08:14:56.847307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text_content(text):\n    text = text.lower()\n#     cleaned_text = re.sub(r\"\"\"\n#                [,.;:@#?!&$*+-=_%<>\\/\\[\\]\\(\\)\\{\\}\\\"\\'\\n]+  # Accept one or more copies of punctuation\n#                \\ *           # plus zero or more copies of a space,\n#                \"\"\",\n#                \" \",          # and replace it with a single space\n#                text, flags=re.VERBOSE)\n#     cleaned_text = re.sub(r\"https?:\\/\\/[A-Za-z0-9.\\/?&#+*+-=_%]+\", \" \",text) #replace urls\n    cleaned_text = re.sub(r\" +\", \" \", text) #remove extra white spaces\n#     cleaned_text = re.sub(r\"[d|D]escription\", \"\", cleaned_text)\n    \n    return cleaned_text","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:14:56.850232Z","iopub.execute_input":"2022-02-09T08:14:56.850605Z","iopub.status.idle":"2022-02-09T08:14:56.858456Z","shell.execute_reply.started":"2022-02-09T08:14:56.850556Z","shell.execute_reply":"2022-02-09T08:14:56.857432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['description_clean']= dataset['description'].apply(lambda x:preprocess_text_content(x))\n# dataset['short_desc1_clean']= dataset['short_desc1'].apply(lambda x:preprocess_text(x))\n# dataset['description2_clean']= dataset['description2'].apply(lambda x:preprocess_text(x))\n# dataset['short_desc2_clean']= dataset['short_desc2'].apply(lambda x:preprocess_text(x))\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:14:56.859653Z","iopub.execute_input":"2022-02-09T08:14:56.861543Z","iopub.status.idle":"2022-02-09T08:15:02.105175Z","shell.execute_reply.started":"2022-02-09T08:14:56.861497Z","shell.execute_reply":"2022-02-09T08:15:02.104373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_column_names()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:02.106449Z","iopub.execute_input":"2022-02-09T08:15:02.107213Z","iopub.status.idle":"2022-02-09T08:15:02.253169Z","shell.execute_reply.started":"2022-02-09T08:15:02.107172Z","shell.execute_reply":"2022-02-09T08:15:02.252436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def preprocess_text(text,tokenizer):\n# #     print(text)\n#     #tokenize the text\n#     text = tokenizer.tokenize(text)\n#     #lowercase the text\n#     text = [w.lower() for w in text]\n#     #removal of stop word \n#     text = [w for w in text if w not in stop_words]\n#     #removal of non-english words\n#     text = [w for w in text if w in words or w.isalpha()]\n    #stemming\n#     text = [ ps.stem(w) for w in text]\n    #remove extra white spaces\n#     text = [ re.sub(r\" +\", \" \", w) for w in text] \n#     cleaned_text = re.sub(r\"\"\"\n#                [,.;:@#?!&$*+-=_%<>\\/\\[\\]\\(\\)\\{\\}\\\"\\'\\n]+  # Accept one or more copies of punctuation\n#                \\ *           # plus zero or more copies of a space,\n#                \"\"\",\n#                \" \",          # and replace it with a single space\n#                text, flags=re.VERBOSE)\n#     cleaned_text = re.sub(r\"https?:\\/\\/[A-Za-z0-9.\\/?&#+*+-=_%]+\", \" \",cleaned_text) #replace urls\n    \n#     cleaned_text = re.sub(r\"[d|D]escription\", \"\", cleaned_text)\n    \n#     return text","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:02.254743Z","iopub.execute_input":"2022-02-09T08:15:02.254978Z","iopub.status.idle":"2022-02-09T08:15:02.262256Z","shell.execute_reply.started":"2022-02-09T08:15:02.254945Z","shell.execute_reply":"2022-02-09T08:15:02.261398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_len = 0\n# lenArr = []\n# outliers = []\n\n# q25, q75 = np.percentile(lenArr, [25, 75])\n# bin_width = 2 * (q75 - q25) * len(lenArr) ** (-1/3)\n# bins = round((lenArr.max() - lenArr.min()) / bin_width)\n\n### Add tokens to the data make it BERT compatible\ndef bert_encode(texts, tokenizer, max_len=512):\n    print('encode')\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n#         text = preprocess_text(text,tokenizer)\n        text = tokenizer.tokenize(text)\n#         print(text)\n        text = [w for w in text if w.lower() not in stop_words]\n    \n#         length = len(text)\n#         if(length <500):\n#             lenArr.append(length)\n#     #     print(length)\n#             if(max_len < length):\n#               max_len = length\n#         else:\n#             outliers.append(length)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n#     print(len(lenArr))\n#     # lenArr=list(filter(lambda a: a != 3395, lenArr))\n#     # print(lenArr)\n#     print(\"avg\", sum(lenArr)/len(lenArr))\n#     print(\"outliers\", outliers)\n#     print(\"outliers\", len(outliers))\n#     plt.hist(lenArr, density=True, bins=30)\n#     max_len\n\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:02.263889Z","iopub.execute_input":"2022-02-09T08:15:02.264284Z","iopub.status.idle":"2022-02-09T08:15:02.274466Z","shell.execute_reply.started":"2022-02-09T08:15:02.264242Z","shell.execute_reply":"2022-02-09T08:15:02.273646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    print('build model')\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    \n#     embedding = bert_layer(\n#         input_word_ids, token_type_ids=segment_ids, attention_mask=input_mask\n#     )[0]\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :] \n#     print(clf_output)\n   \n#     clf_output = Reshape((1024,1))(clf_output)\n#     print(clf_output)\n    \n#     x = Conv1D(8, 3, activation='relu', padding='same')(clf_output)\n#     x = BatchNormalization()(x)\n#     x = MaxPooling1D()(x)\n#     x = Conv1D(32, 3, activation='relu', padding='same')(x)\n#     x = BatchNormalization()(x)\n#     x = MaxPooling1D()(x)\n#     x = Flatten()(x)\n#     out = Dense(1, activation='sigmoid')(x)\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:02.275635Z","iopub.execute_input":"2022-02-09T08:15:02.276502Z","iopub.status.idle":"2022-02-09T08:15:02.286344Z","shell.execute_reply.started":"2022-02-09T08:15:02.276461Z","shell.execute_reply":"2022-02-09T08:15:02.285579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train, test = train_test_split(dataset, test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:02.287374Z","iopub.execute_input":"2022-02-09T08:15:02.288098Z","iopub.status.idle":"2022-02-09T08:15:02.297989Z","shell.execute_reply.started":"2022-02-09T08:15:02.288059Z","shell.execute_reply":"2022-02-09T08:15:02.297292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FullTokenizer = tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\", trainable=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:02.299459Z","iopub.execute_input":"2022-02-09T08:15:02.299969Z","iopub.status.idle":"2022-02-09T08:15:30.049273Z","shell.execute_reply.started":"2022-02-09T08:15:02.299931Z","shell.execute_reply":"2022-02-09T08:15:30.04855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nlower_cased = bert_layer.resolved_object.do_lower_case.numpy()\n\ntokenizer = FullTokenizer(vocab_file, lower_cased)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:30.050648Z","iopub.execute_input":"2022-02-09T08:15:30.050896Z","iopub.status.idle":"2022-02-09T08:15:30.161389Z","shell.execute_reply.started":"2022-02-09T08:15:30.050862Z","shell.execute_reply":"2022-02-09T08:15:30.160621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_input = bert_encode(dataset['description'], tokenizer, max_len=160)\n# test_input = bert_encode(test['description'], tokenizer, max_len=160)\n# train_labels = np.array(train.duplicate.values, dtype='int')\n# test_labels = np.array(test.duplicate.values, dtype='int')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:30.162744Z","iopub.execute_input":"2022-02-09T08:15:30.162987Z","iopub.status.idle":"2022-02-09T08:15:30.166828Z","shell.execute_reply.started":"2022-02-09T08:15:30.162953Z","shell.execute_reply":"2022-02-09T08:15:30.165841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_len = 0\n# lenArr = []\n# outliers = []\n\n# for index, row in dataset.iterrows():\n#     # print(train.loc[[index]]['description'])\n#     length = max([len(s.split()) for s in dataset.loc[[index]]['description']])\n#     if(length <1000):\n#         lenArr.append(length)\n# #     print(length)\n#         if(max_len < length):\n#           max_len = length\n#     else:\n#         outliers.append(length)\n\n# # q25, q75 = np.percentile(lenArr, [25, 75])\n# # bin_width = 2 * (q75 - q25) * len(lenArr) ** (-1/3)\n# # bins = round((lenArr.max() - lenArr.min()) / bin_width)\n# print(len(lenArr))\n# # lenArr=list(filter(lambda a: a != 3395, lenArr))\n# # print(lenArr)\n# print(\"avg\", sum(lenArr)/len(lenArr))\n# print(\"outliers\", outliers)\n# print(\"outliers\", len(outliers))\n# plt.hist(lenArr, density=True, bins=30)\n# max_len","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:30.168183Z","iopub.execute_input":"2022-02-09T08:15:30.168653Z","iopub.status.idle":"2022-02-09T08:15:30.176471Z","shell.execute_reply.started":"2022-02-09T08:15:30.168615Z","shell.execute_reply":"2022-02-09T08:15:30.175761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = build_model(bert_layer, max_len=160)\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:30.177737Z","iopub.execute_input":"2022-02-09T08:15:30.178119Z","iopub.status.idle":"2022-02-09T08:15:30.187648Z","shell.execute_reply.started":"2022-02-09T08:15:30.178044Z","shell.execute_reply":"2022-02-09T08:15:30.186877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_history = model.fit(\n#     train_input, train_labels,\n#     validation_split=0.3,\n#     epochs=5,\n#     batch_size=50\n# )","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:30.189805Z","iopub.execute_input":"2022-02-09T08:15:30.190629Z","iopub.status.idle":"2022-02-09T08:15:30.195971Z","shell.execute_reply.started":"2022-02-09T08:15:30.190589Z","shell.execute_reply":"2022-02-09T08:15:30.195224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluation_results = model.evaluate(test_input,test_labels,return_dict=True)\n# evaluation_results","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:15:30.197501Z","iopub.execute_input":"2022-02-09T08:15:30.19779Z","iopub.status.idle":"2022-02-09T08:15:30.203947Z","shell.execute_reply.started":"2022-02-09T08:15:30.197719Z","shell.execute_reply":"2022-02-09T08:15:30.203288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dataset['description']\nY = np.array(dataset.duplicate.values, dtype='int')\n\nkfold = StratifiedKFold()\ncvscores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\n    \nfor train, test in kfold.split(X, Y):\n    # Prepare data\n    train_input = bert_encode(X.iloc[train], tokenizer, max_len=160)\n    test_input = bert_encode(X.iloc[test], tokenizer, max_len=160)\n    train_labels = Y[train]\n    test_labels = Y[test]\n    \n    model = build_model(bert_layer, max_len=160)\n    model.summary()\n\n    # Fit the model\n    model.fit(train_input,train_labels, epochs=1, batch_size=15)\n\n    # Evaluate the model\n    scores = model.evaluate(test_input, test_labels, verbose=0)\n    f1score = (2 * scores[2]*100 * scores[3]*100)/(scores[2]*100 + scores[3]*100)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n    print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n    print(\"f1 score: \", (f1score))\n    cvscores.append(scores[1] * 100)\n    precision_scores.append(scores[2] * 100)\n    recall_scores.append(scores[3] * 100)\n    f1_scores.append(f1score)\n    \n\nprint(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\nprint(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision_scores), np.std(precision_scores)))\nprint(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall_scores), np.std(recall_scores)))\nprint(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-02-09T08:15:30.205311Z","iopub.execute_input":"2022-02-09T08:15:30.205629Z"},"trusted":true},"execution_count":null,"outputs":[]}]}