{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-19T02:51:57.569345Z","iopub.execute_input":"2022-02-19T02:51:57.569844Z","iopub.status.idle":"2022-02-19T02:51:57.603822Z","shell.execute_reply.started":"2022-02-19T02:51:57.569753Z","shell.execute_reply":"2022-02-19T02:51:57.603137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom random import seed\nfrom random import randint\nimport random\nimport string\nimport re\nimport matplotlib.pyplot as \n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:51:57.605488Z","iopub.execute_input":"2022-02-19T02:51:57.605741Z","iopub.status.idle":"2022-02-19T02:51:57.610322Z","shell.execute_reply.started":"2022-02-19T02:51:57.605707Z","shell.execute_reply":"2022-02-19T02:51:57.609464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport transformers\n\nimport nltk\n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nplt.style.use('seaborn')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:51:57.611531Z","iopub.execute_input":"2022-02-19T02:51:57.612073Z","iopub.status.idle":"2022-02-19T02:52:00.364002Z","shell.execute_reply.started":"2022-02-19T02:51:57.612035Z","shell.execute_reply":"2022-02-19T02:52:00.363271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:00.366075Z","iopub.execute_input":"2022-02-19T02:52:00.36633Z","iopub.status.idle":"2022-02-19T02:52:00.546792Z","shell.execute_reply.started":"2022-02-19T02:52:00.366294Z","shell.execute_reply":"2022-02-19T02:52:00.545465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, Reshape, Conv1D, Conv2D, BatchNormalization, MaxPooling1D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, Sequential\n# from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:00.547946Z","iopub.execute_input":"2022-02-19T02:52:00.550774Z","iopub.status.idle":"2022-02-19T02:52:05.588646Z","shell.execute_reply.started":"2022-02-19T02:52:00.550744Z","shell.execute_reply":"2022-02-19T02:52:05.587866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed(1)\nstop_words = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:05.589828Z","iopub.execute_input":"2022-02-19T02:52:05.591726Z","iopub.status.idle":"2022-02-19T02:52:05.598047Z","shell.execute_reply.started":"2022-02-19T02:52:05.591695Z","shell.execute_reply":"2022-02-19T02:52:05.597235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:05.599695Z","iopub.execute_input":"2022-02-19T02:52:05.59995Z","iopub.status.idle":"2022-02-19T02:52:05.650529Z","shell.execute_reply.started":"2022-02-19T02:52:05.599915Z","shell.execute_reply":"2022-02-19T02:52:05.649798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH_CSV_TRAIN = '../input/eclipse-preprocessed5new/eclipse_preprocessed_os.csv'\n# PATH_CSV_TEST = '../input/nlpgettingstarted/test.csv'\n# PATH_CSV_SUBMISSION = '../input/nlpgettingstarted/sample_submission.csv'\n\ndataset = pd.read_csv(PATH_CSV_TRAIN)\n# dataf_test = pd.read_csv(PATH_CSV_TEST)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:05.651914Z","iopub.execute_input":"2022-02-19T02:52:05.652152Z","iopub.status.idle":"2022-02-19T02:52:14.755235Z","shell.execute_reply.started":"2022-02-19T02:52:05.652119Z","shell.execute_reply":"2022-02-19T02:52:14.754471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.dropna(subset = [\"description1\"], inplace=True)\ndataset.dropna(subset = [\"description2\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:14.75658Z","iopub.execute_input":"2022-02-19T02:52:14.75701Z","iopub.status.idle":"2022-02-19T02:52:14.94048Z","shell.execute_reply.started":"2022-02-19T02:52:14.756963Z","shell.execute_reply":"2022-02-19T02:52:14.939639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reset_column_names():\n  dataset.drop('description1', axis=1, inplace=True)\n  dataset.drop('short_desc1', axis=1, inplace=True)\n  dataset.drop('description2', axis=1, inplace=True)\n  dataset.drop('short_desc2', axis=1, inplace=True)\n\n  dataset.rename(columns={'description1_clean':'description1','short_desc1_clean':'short_desc1','description2_clean':'description2','short_desc2_clean':'short_desc2'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:14.943725Z","iopub.execute_input":"2022-02-19T02:52:14.943999Z","iopub.status.idle":"2022-02-19T02:52:14.94894Z","shell.execute_reply.started":"2022-02-19T02:52:14.943952Z","shell.execute_reply":"2022-02-19T02:52:14.948293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    #Remove emojis and special chars\n    clean=text\n    #reg = re.compile('\\\\.+?(?=\\B|$)')\n    #clean = text.apply(lambda r: re.sub(reg, string=r, repl=''))\n    #reg = re.compile('\\x89Ã›_')\n    #clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n#     reg = re.compile('\\&amp')\n#     clean = clean.apply(lambda r: re.sub(reg, string=r, repl='&'))\n#     reg = re.compile('\\\\n')\n#     clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n\n    #Remove hashtag symbol (#)\n    #clean = clean.apply(lambda r: r.replace('#', ''))\n\n    #Remove user names\n#     reg = re.compile('@[a-zA-Z0-9\\_]+')\n#     clean = clean.apply(lambda r: re.sub(reg, string=r, repl='@'))\n\n    #Remove URLs\n#     reg = re.compile(r\"https?:\\/\\/[A-Za-z0-9.\\/?&#+*+-=_%]+\")\n#     clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n    \n#     reg = re.compile(r\"\"\"\n#                [,.;:@#?!&$*+-=_%<>\\/\\[\\]\\(\\)\\{\\}\\\"\\'\\n]+  # Accept one or more copies of punctuation\n#                \\ *           # plus zero or more copies of a space,\n#                \"\"\")\n#     clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n    \n    reg = re.compile(r\" +\")\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n    \n#     reg = re.compile(r\"[d|D]escription\")\n#     clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n    \n\n    #Lowercase\n    clean = clean.apply(lambda r: r.lower())\n    return clean","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:14.95025Z","iopub.execute_input":"2022-02-19T02:52:14.950642Z","iopub.status.idle":"2022-02-19T02:52:14.958978Z","shell.execute_reply.started":"2022-02-19T02:52:14.950606Z","shell.execute_reply":"2022-02-19T02:52:14.958315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['description1_clean'] = clean_text(dataset['description1'])\ndataset['short_desc1_clean'] = clean_text(dataset['short_desc1'])\ndataset['description2_clean'] = clean_text(dataset['description2'])\ndataset['short_desc2_clean'] = clean_text(dataset['short_desc2'])\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:14.960226Z","iopub.execute_input":"2022-02-19T02:52:14.960495Z","iopub.status.idle":"2022-02-19T02:52:33.298949Z","shell.execute_reply.started":"2022-02-19T02:52:14.960461Z","shell.execute_reply":"2022-02-19T02:52:33.298266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_column_names()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:33.300128Z","iopub.execute_input":"2022-02-19T02:52:33.300495Z","iopub.status.idle":"2022-02-19T02:52:33.634636Z","shell.execute_reply.started":"2022-02-19T02:52:33.300454Z","shell.execute_reply":"2022-02-19T02:52:33.633898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:33.63603Z","iopub.execute_input":"2022-02-19T02:52:33.63629Z","iopub.status.idle":"2022-02-19T02:52:33.658115Z","shell.execute_reply.started":"2022-02-19T02:52:33.636256Z","shell.execute_reply":"2022-02-19T02:52:33.657459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def max_length(lines):\n#     return max([len(s.split()) for s in lines])","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:33.659474Z","iopub.execute_input":"2022-02-19T02:52:33.659693Z","iopub.status.idle":"2022-02-19T02:52:33.664261Z","shell.execute_reply.started":"2022-02-19T02:52:33.659661Z","shell.execute_reply":"2022-02-19T02:52:33.663451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in dataset.iterrows():\n    desc = row['description1'] + row['description2']\n    dataset.loc[index, 'description'] = desc","metadata":{"execution":{"iopub.status.busy":"2022-02-19T02:52:33.665636Z","iopub.execute_input":"2022-02-19T02:52:33.666049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFXLNetModel, XLNetTokenizer, XLNetConfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xlnet_model = 'xlnet-base-cased'\nxlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_xlnet(mname):\n    \"\"\" Creates the model. It is composed of the XLNet main block and then\n    a classification head its added\n    \"\"\"\n    # Define token ids as inputs\n    word_inputs = tf.keras.Input(shape=(160,), name='word_inputs', dtype='int32')\n\n    # Call XLNet model\n    xlnet = TFXLNetModel.from_pretrained(mname)\n    xlnet_encodings = xlnet(word_inputs)[0]\n\n    # CLASSIFICATION HEAD \n    # Collect last step from last hidden state (CLS)\n    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n    # Apply dropout for regularization\n    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n    # Final output \n    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(doc_encoding)\n\n    # Compile model\n    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xlnet = create_xlnet(xlnet_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xlnet.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inp_descriptions = dataset['description']\n# labels = dataset['duplicate']\n\n# X_train, X_test, y_train, y_test = train_test_split(inp_descriptions, labels, test_size=0.15, random_state=196)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_inputs(tweets, tokenizer, max_len=160):\n    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in tweets]\n    inp_tok = np.array([a['input_ids'] for a in inps])\n    ids = np.array([a['attention_mask'] for a in inps])\n    segments = np.array([a['token_type_ids'] for a in inps])\n    return inp_tok, ids, segments","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dataset['description']\nY = np.array(dataset.duplicate.values, dtype='int')\n# X = dataset['description']\n# Y = dataset['duplicate']\n\nkfold = StratifiedKFold()\ncvscores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\n    \nfor train, test in kfold.split(X, Y):\n#     print(X.iloc[train].shape)\n#     print(X.iloc[train])\n#     print(Y[train])\n\n    # Prepare data\n    inp_tok, ids, segments = get_inputs(X.iloc[train], xlnet_tokenizer)\n    y_train = np.array(Y[train], dtype='int')\n#     print(inp_tok, ids, segments)\n#     print(y_train)\n    \n    xlnet = create_xlnet(xlnet_model)\n    xlnet.summary()\n    # Fit the model  \n    hist = xlnet.fit(x=inp_tok, y=y_train, epochs=5, batch_size=40)\n    \n    test_inp_tok, test_ids, test_segments = get_inputs(X.iloc[test], xlnet_tokenizer)\n    y_test = np.array(Y[test], dtype='int')\n\n    # Evaluate the model\n    scores = xlnet.evaluate(test_inp_tok,y_test, verbose=0)\n    f1score = (2 * scores[2]*100 * scores[3]*100)/(scores[2]*100 + scores[3]*100)\n    print(\"%s: %.2f%%\" % (xlnet.metrics_names[1], scores[1]*100))\n    print(\"%s: %.2f%%\" % (xlnet.metrics_names[2], scores[2]*100))\n    print(\"%s: %.2f%%\" % (xlnet.metrics_names[3], scores[3]*100))\n    print(\"f1 score: \", (f1score))\n    cvscores.append(scores[1] * 100)\n    precision_scores.append(scores[2] * 100)\n    recall_scores.append(scores[3] * 100)\n    f1_scores.append(f1score)\n\nprint(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\nprint(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision_scores), np.std(precision_scores)))\nprint(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall_scores), np.std(recall_scores)))\nprint(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inp_tok, ids, segments = get_inputs(X_train, xlnet_tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callbacks = [\n#     tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.02, restore_best_weights=True),\n#     tf.keras.callbacks.LearningRateScheduler(warmup, verbose=0),\n#     tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=1e-6, patience=2, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-6)\n# ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_train = np.array(y_train, dtype='int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hist = xlnet.fit(x=inp_tok, y=y_train, epochs=5, batch_size=16, validation_split=.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inp_tok, ids, segments = get_inputs(X_test, xlnet_tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_test = np.array(y_test, dtype='int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluation_results = xlnet.evaluate(inp_tok,y_test,return_dict=True)\n# evaluation_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = xlnet.predict(inp_tok, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_metrics(preds, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}