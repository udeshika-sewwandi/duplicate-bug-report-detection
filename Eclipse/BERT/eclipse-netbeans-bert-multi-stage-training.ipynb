{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-20T12:01:04.024816Z","iopub.execute_input":"2022-03-20T12:01:04.025536Z","iopub.status.idle":"2022-03-20T12:01:04.098645Z","shell.execute_reply.started":"2022-03-20T12:01:04.025438Z","shell.execute_reply":"2022-03-20T12:01:04.097708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/tools/tokenization.py","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:04.100869Z","iopub.execute_input":"2022-03-20T12:01:04.101582Z","iopub.status.idle":"2022-03-20T12:01:05.210514Z","shell.execute_reply.started":"2022-03-20T12:01:04.10154Z","shell.execute_reply":"2022-03-20T12:01:05.209256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:05.212727Z","iopub.execute_input":"2022-03-20T12:01:05.213282Z","iopub.status.idle":"2022-03-20T12:01:16.097548Z","shell.execute_reply.started":"2022-03-20T12:01:05.213238Z","shell.execute_reply":"2022-03-20T12:01:16.096421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_hub","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:16.099775Z","iopub.execute_input":"2022-03-20T12:01:16.100102Z","iopub.status.idle":"2022-03-20T12:01:25.942021Z","shell.execute_reply.started":"2022-03-20T12:01:16.100057Z","shell.execute_reply":"2022-03-20T12:01:25.94089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:25.947344Z","iopub.execute_input":"2022-03-20T12:01:25.947626Z","iopub.status.idle":"2022-03-20T12:01:35.184421Z","shell.execute_reply.started":"2022-03-20T12:01:25.94759Z","shell.execute_reply":"2022-03-20T12:01:35.183096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download(\"stopwords\")\nnltk.download(\"words\")\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import words\n# words = set(nltk.corpus.words.words())","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:35.186763Z","iopub.execute_input":"2022-03-20T12:01:35.187133Z","iopub.status.idle":"2022-03-20T12:01:36.982638Z","shell.execute_reply.started":"2022-03-20T12:01:35.187081Z","shell.execute_reply":"2022-03-20T12:01:36.981436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom random import seed\nfrom random import randint\nimport random\nimport string\nimport re\nimport matplotlib.pyplot as plt\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:36.986196Z","iopub.execute_input":"2022-03-20T12:01:36.986459Z","iopub.status.idle":"2022-03-20T12:01:36.993504Z","shell.execute_reply.started":"2022-03-20T12:01:36.986429Z","shell.execute_reply":"2022-03-20T12:01:36.992352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, Reshape, Conv1D, Conv2D, BatchNormalization, MaxPooling1D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, Sequential\n# from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\n# from bert import bert_tokenization\nimport tokenization\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:36.995734Z","iopub.execute_input":"2022-03-20T12:01:36.996236Z","iopub.status.idle":"2022-03-20T12:01:43.25489Z","shell.execute_reply.started":"2022-03-20T12:01:36.996093Z","shell.execute_reply":"2022-03-20T12:01:43.253851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\nsession = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:43.256269Z","iopub.execute_input":"2022-03-20T12:01:43.256628Z","iopub.status.idle":"2022-03-20T12:01:45.818128Z","shell.execute_reply.started":"2022-03-20T12:01:43.256578Z","shell.execute_reply":"2022-03-20T12:01:45.817094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed(1)\nstop_words = stopwords.words('english')\nwords=words.words()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:45.819409Z","iopub.execute_input":"2022-03-20T12:01:45.819757Z","iopub.status.idle":"2022-03-20T12:01:45.919853Z","shell.execute_reply.started":"2022-03-20T12:01:45.819716Z","shell.execute_reply":"2022-03-20T12:01:45.918816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_indices = []\n# test_indices = []\n\n# with open('../input/netbeans-train-test5/nb_train5.txt') as f:\n#     train_indices = f.readlines()\n\n# with open('../input/netbeans-train-test5/nb_test5.txt') as f:\n#     test_indices = f.readlines()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:45.921306Z","iopub.execute_input":"2022-03-20T12:01:45.922828Z","iopub.status.idle":"2022-03-20T12:01:45.965355Z","shell.execute_reply.started":"2022-03-20T12:01:45.922777Z","shell.execute_reply":"2022-03-20T12:01:45.964325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_indices = [int(tr.split('\\n')[0]) for tr in train_indices]","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:45.966799Z","iopub.execute_input":"2022-03-20T12:01:45.967136Z","iopub.status.idle":"2022-03-20T12:01:46.04435Z","shell.execute_reply.started":"2022-03-20T12:01:45.967092Z","shell.execute_reply":"2022-03-20T12:01:46.04334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(train_indices)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:46.048115Z","iopub.execute_input":"2022-03-20T12:01:46.049533Z","iopub.status.idle":"2022-03-20T12:01:46.06031Z","shell.execute_reply.started":"2022-03-20T12:01:46.049479Z","shell.execute_reply":"2022-03-20T12:01:46.059173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_indices  = [int(te.split('\\n')[0]) for te in test_indices]","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:46.066299Z","iopub.execute_input":"2022-03-20T12:01:46.066637Z","iopub.status.idle":"2022-03-20T12:01:46.091866Z","shell.execute_reply.started":"2022-03-20T12:01:46.066577Z","shell.execute_reply":"2022-03-20T12:01:46.090907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(test_indices)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:46.093427Z","iopub.execute_input":"2022-03-20T12:01:46.093765Z","iopub.status.idle":"2022-03-20T12:01:46.10167Z","shell.execute_reply.started":"2022-03-20T12:01:46.093721Z","shell.execute_reply":"2022-03-20T12:01:46.100422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('../input/netbeans-preprocessed5/netbeans_preprocessed_os.csv')\ndataset","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:01:46.103663Z","iopub.execute_input":"2022-03-20T12:01:46.104437Z","iopub.status.idle":"2022-03-20T12:02:05.135546Z","shell.execute_reply.started":"2022-03-20T12:01:46.104389Z","shell.execute_reply":"2022-03-20T12:02:05.134423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.dropna(subset = [\"description1\"], inplace=True)\ndataset.dropna(subset = [\"description2\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:02:05.137093Z","iopub.execute_input":"2022-03-20T12:02:05.138639Z","iopub.status.idle":"2022-03-20T12:02:05.266895Z","shell.execute_reply.started":"2022-03-20T12:02:05.138593Z","shell.execute_reply":"2022-03-20T12:02:05.265775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in dataset.iterrows():\n    doc1 = 'Product:' + row['product'] + '.Component:' + row['component'] + '.Description:' + row['description1'] #+ '.Summary:' + row['short_desc1']\n#     print(row['bug_id'])\n#     print(row['description2'])\n    doc2 = 'Product:' + row['product'] + '.Component:' + row['component'] + '.Description:' + row['description2'] #+ '.Summary:' + row['short_desc2']\n    dataset.loc[index, 'doc1'] = doc1\n    dataset.loc[index, 'doc2'] = doc2","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:02:05.268681Z","iopub.execute_input":"2022-03-20T12:02:05.268978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in dataset.iterrows():\n    desc = row['doc1'] + row['doc2']\n    dataset.loc[index, 'description'] = desc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reset_column_names():\n  dataset.drop('description', axis=1, inplace=True)\n#   dataset.drop('short_desc1', axis=1, inplace=True)\n#   dataset.drop('description2', axis=1, inplace=True)\n#   dataset.drop('short_desc2', axis=1, inplace=True)\n\n  dataset.rename(columns={'description_clean':'description'}, inplace=True) #,'short_desc1_clean':'short_desc1','description2_clean':'description2','short_desc2_clean':'short_desc2'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text_content(text):\n    text = text.lower()\n#     cleaned_text = re.sub(r\"\"\"\n#                [,.;:@#?!&$*+-=_%<>\\/\\[\\]\\(\\)\\{\\}\\\"\\'\\n]+  # Accept one or more copies of punctuation\n#                \\ *           # plus zero or more copies of a space,\n#                \"\"\",\n#                \" \",          # and replace it with a single space\n#                text, flags=re.VERBOSE)\n#     cleaned_text = re.sub(r\"https?:\\/\\/[A-Za-z0-9.\\/?&#+*+-=_%]+\", \" \",text) #replace urls\n    cleaned_text = re.sub(r\" +\", \" \", text) #remove extra white spaces\n#     cleaned_text = re.sub(r\"[d|D]escription\", \"\", cleaned_text)\n    \n    return cleaned_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['description_clean']= dataset['description'].apply(lambda x:preprocess_text_content(x))\n# dataset['short_desc1_clean']= dataset['short_desc1'].apply(lambda x:preprocess_text(x))\n# dataset['description2_clean']= dataset['description2'].apply(lambda x:preprocess_text(x))\n# dataset['short_desc2_clean']= dataset['short_desc2'].apply(lambda x:preprocess_text(x))\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_column_names()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def preprocess_text(text,tokenizer):\n# #     print(text)\n#     #tokenize the text\n#     text = tokenizer.tokenize(text)\n#     #lowercase the text\n#     text = [w.lower() for w in text]\n#     #removal of stop word \n#     text = [w for w in text if w not in stop_words]\n#     #removal of non-english words\n#     text = [w for w in text if w in words or w.isalpha()]\n    #stemming\n#     text = [ ps.stem(w) for w in text]\n    #remove extra white spaces\n#     text = [ re.sub(r\" +\", \" \", w) for w in text] \n#     cleaned_text = re.sub(r\"\"\"\n#                [,.;:@#?!&$*+-=_%<>\\/\\[\\]\\(\\)\\{\\}\\\"\\'\\n]+  # Accept one or more copies of punctuation\n#                \\ *           # plus zero or more copies of a space,\n#                \"\"\",\n#                \" \",          # and replace it with a single space\n#                text, flags=re.VERBOSE)\n#     cleaned_text = re.sub(r\"https?:\\/\\/[A-Za-z0-9.\\/?&#+*+-=_%]+\", \" \",cleaned_text) #replace urls\n    \n#     cleaned_text = re.sub(r\"[d|D]escription\", \"\", cleaned_text)\n    \n#     return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0\nlenArr = []\noutliers = []\n\n# q25, q75 = np.percentile(lenArr, [25, 75])\n# bin_width = 2 * (q75 - q25) * len(lenArr) ** (-1/3)\n# bins = round((lenArr.max() - lenArr.min()) / bin_width)\n\n### Add tokens to the data make it BERT compatible\ndef bert_encode(texts, tokenizer, max_len=512):\n    print('encode')\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n#         text = preprocess_text(text,tokenizer)\n        text = tokenizer.tokenize(text)\n#         print(text)\n        text = [w for w in text if w.lower() not in stop_words]\n    \n#         length = len(text)\n#         if(length <1000):\n#             lenArr.append(length)\n#     #     print(length)\n#             if(max_len < length):\n#               max_len = length\n#         else:\n#             outliers.append(length)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n#     print(len(lenArr))\n#     # lenArr=list(filter(lambda a: a != 3395, lenArr))\n#     # print(lenArr)\n#     print(\"avg\", sum(lenArr)/len(lenArr))\n#     print(\"outliers\", outliers)\n#     print(\"outliers\", len(outliers))\n#     plt.hist(lenArr, density=True, bins=30)\n#     max_len\n\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    print('build model')\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    \n#     embedding = bert_layer(\n#         input_word_ids, token_type_ids=segment_ids, attention_mask=input_mask\n#     )[0]\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :] \n#     print(clf_output)\n   \n#     clf_output = Reshape((1024,1))(clf_output)\n#     print(clf_output)\n    \n#     x = Conv1D(8, 3, activation='relu', padding='same')(clf_output)\n#     x = BatchNormalization()(x)\n#     x = MaxPooling1D()(x)\n#     x = Conv1D(32, 3, activation='relu', padding='same')(x)\n#     x = BatchNormalization()(x)\n#     x = MaxPooling1D()(x)\n#     x = Flatten()(x)\n#     out = Dense(1, activation='sigmoid')(x)\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(dataset, test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FullTokenizer = tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\", trainable=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nlower_cased = bert_layer.resolved_object.do_lower_case.numpy()\n\ntokenizer = FullTokenizer(vocab_file, lower_cased)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X = dataset['description']\n# Y = np.array(dataset.duplicate.values, dtype='int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input = bert_encode(train['description'], tokenizer, max_len=160)\ntrain_labels = np.array(train.duplicate.values, dtype='int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = build_model(bert_layer, max_len=160)\nmodel = load_model(\"../input/eclipse-bert-model/eclipse_bert\")\nmodel.summary()\n\n# Fit the model\nmodel.fit(train_input,train_labels, epochs=4, batch_size=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('eclipse_netbeans_model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#netbeans testing\ntest_input = bert_encode(test['description'], tokenizer, max_len=160)\ntest_labels = np.array(test.duplicate.values, dtype='int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.evaluate(test_input, test_labels, verbose=0)\nf1score = (2 * scores[2]*100 * scores[3]*100)/(scores[2]*100 + scores[3]*100)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\nprint(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\nprint(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\nprint(\"f1 score: \", (f1score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_input = bert_encode(dataset['description'], tokenizer, max_len=160)\n# test_input = bert_encode(test['description'], tokenizer, max_len=160)\n# train_labels = np.array(train.duplicate.values, dtype='int')\n# test_labels = np.array(test.duplicate.values, dtype='int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_len = 0\n# lenArr = []\n# outliers = []\n\n# for index, row in dataset.iterrows():\n#     # print(train.loc[[index]]['description'])\n#     length = max([len(s.split()) for s in dataset.loc[[index]]['description']])\n#     if(length <1000):\n#         lenArr.append(length)\n# #     print(length)\n#         if(max_len < length):\n#           max_len = length\n#     else:\n#         outliers.append(length)\n\n# # q25, q75 = np.percentile(lenArr, [25, 75])\n# # bin_width = 2 * (q75 - q25) * len(lenArr) ** (-1/3)\n# # bins = round((lenArr.max() - lenArr.min()) / bin_width)\n# print(len(lenArr))\n# # lenArr=list(filter(lambda a: a != 3395, lenArr))\n# # print(lenArr)\n# print(\"avg\", sum(lenArr)/len(lenArr))\n# print(\"outliers\", outliers)\n# print(\"outliers\", len(outliers))\n# plt.hist(lenArr, density=True, bins=30)\n# max_len","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = build_model(bert_layer, max_len=160)\n# model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_history = model.fit(\n#     train_input, train_labels,\n#     validation_split=0.3,\n#     epochs=5,\n#     batch_size=50\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluation_results = model.evaluate(test_input,test_labels,return_dict=True)\n# evaluation_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X = dataset['description']\n# Y = np.array(dataset.duplicate.values, dtype='int')\n\n# kfold = StratifiedKFold()\n# cvscores = []\n# precision_scores = []\n# recall_scores = []\n# f1_scores = []\n    \n# for train, test in kfold.split(X, Y):\n#     # Prepare data\n#     train_input = bert_encode(X.iloc[train], tokenizer, max_len=160)\n#     test_input = bert_encode(X.iloc[test], tokenizer, max_len=160)\n#     train_labels = Y[train]\n#     test_labels = Y[test]\n    \n#     model = build_model(bert_layer, max_len=160)\n#     model.summary()\n\n#     # Fit the model\n#     model.fit(train_input,train_labels, epochs=2, batch_size=15)\n\n#     # Evaluate the model\n#     scores = model.evaluate(test_input, test_labels, verbose=0)\n#     f1score = (2 * scores[2]*100 * scores[3]*100)/(scores[2]*100 + scores[3]*100)\n#     print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n#     print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n#     print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n#     print(\"f1 score: \", (f1score))\n#     cvscores.append(scores[1] * 100)\n#     precision_scores.append(scores[2] * 100)\n#     recall_scores.append(scores[3] * 100)\n#     f1_scores.append(f1score)\n    \n\n# print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n# print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision_scores), np.std(precision_scores)))\n# print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall_scores), np.std(recall_scores)))\n# print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))","metadata":{"_kg_hide-output":false,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]}]}