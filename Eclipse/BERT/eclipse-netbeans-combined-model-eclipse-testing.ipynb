{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-28T17:30:53.085344Z","iopub.execute_input":"2022-03-28T17:30:53.086622Z","iopub.status.idle":"2022-03-28T17:30:53.115814Z","shell.execute_reply.started":"2022-03-28T17:30:53.086583Z","shell.execute_reply":"2022-03-28T17:30:53.115128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/tools/tokenization.py","metadata":{"execution":{"iopub.status.busy":"2022-03-28T17:30:53.117021Z","iopub.execute_input":"2022-03-28T17:30:53.117266Z","iopub.status.idle":"2022-03-28T17:30:54.085035Z","shell.execute_reply.started":"2022-03-28T17:30:53.117234Z","shell.execute_reply":"2022-03-28T17:30:54.084177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2022-03-28T17:30:54.087385Z","iopub.execute_input":"2022-03-28T17:30:54.087756Z","iopub.status.idle":"2022-03-28T17:31:02.729109Z","shell.execute_reply.started":"2022-03-28T17:30:54.087717Z","shell.execute_reply":"2022-03-28T17:31:02.728203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_hub","metadata":{"execution":{"iopub.status.busy":"2022-03-28T17:31:02.732642Z","iopub.execute_input":"2022-03-28T17:31:02.732868Z","iopub.status.idle":"2022-03-28T17:31:09.990378Z","shell.execute_reply.started":"2022-03-28T17:31:02.732835Z","shell.execute_reply":"2022-03-28T17:31:09.989546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:06.096459Z","iopub.execute_input":"2022-03-28T00:24:06.096842Z","iopub.status.idle":"2022-03-28T00:24:13.224410Z","shell.execute_reply.started":"2022-03-28T00:24:06.096796Z","shell.execute_reply":"2022-03-28T00:24:13.223559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download(\"stopwords\")\nnltk.download(\"words\")\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import words\n# words = set(nltk.corpus.words.words())","metadata":{"execution":{"iopub.status.busy":"2022-03-28T17:31:09.992131Z","iopub.execute_input":"2022-03-28T17:31:09.992415Z","iopub.status.idle":"2022-03-28T17:31:11.698039Z","shell.execute_reply.started":"2022-03-28T17:31:09.992385Z","shell.execute_reply":"2022-03-28T17:31:11.697198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom random import seed\nfrom random import randint\nimport random\nimport string\nimport re\nimport matplotlib.pyplot as plt\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T17:31:11.699414Z","iopub.execute_input":"2022-03-28T17:31:11.699894Z","iopub.status.idle":"2022-03-28T17:31:11.704945Z","shell.execute_reply.started":"2022-03-28T17:31:11.699852Z","shell.execute_reply":"2022-03-28T17:31:11.704127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, Reshape, Conv1D, Conv2D, BatchNormalization, MaxPooling1D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, Sequential, load_model\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport tokenization\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:14.867621Z","iopub.execute_input":"2022-03-28T00:24:14.868091Z","iopub.status.idle":"2022-03-28T00:24:20.032891Z","shell.execute_reply.started":"2022-03-28T00:24:14.868054Z","shell.execute_reply":"2022-03-28T00:24:20.032115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\nsession = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:20.034380Z","iopub.execute_input":"2022-03-28T00:24:20.034623Z","iopub.status.idle":"2022-03-28T00:24:21.983551Z","shell.execute_reply.started":"2022-03-28T00:24:20.034589Z","shell.execute_reply":"2022-03-28T00:24:21.981865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed(1)\nstop_words = stopwords.words('english')\nwords=words.words()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:21.986904Z","iopub.execute_input":"2022-03-28T00:24:21.987263Z","iopub.status.idle":"2022-03-28T00:24:22.072161Z","shell.execute_reply.started":"2022-03-28T00:24:21.987230Z","shell.execute_reply":"2022-03-28T00:24:22.071382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_indices = []\n# test_indices = []\n\n# with open('../input/eclipse-train-test5/train5_dt5.txt') as f:\n#     train_indices = f.readlines()\n\n# with open('../input/eclipse-train-test5/test5_dt5.txt') as f:\n#     test_indices = f.readlines()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:22.073319Z","iopub.execute_input":"2022-03-28T00:24:22.073588Z","iopub.status.idle":"2022-03-28T00:24:22.079561Z","shell.execute_reply.started":"2022-03-28T00:24:22.073551Z","shell.execute_reply":"2022-03-28T00:24:22.076979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_indices = [int(tr.split('\\n')[0]) for tr in train_indices]","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:22.080820Z","iopub.execute_input":"2022-03-28T00:24:22.081552Z","iopub.status.idle":"2022-03-28T00:24:22.085629Z","shell.execute_reply.started":"2022-03-28T00:24:22.081516Z","shell.execute_reply":"2022-03-28T00:24:22.084889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(train_indices)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:22.086751Z","iopub.execute_input":"2022-03-28T00:24:22.087528Z","iopub.status.idle":"2022-03-28T00:24:22.094430Z","shell.execute_reply.started":"2022-03-28T00:24:22.087491Z","shell.execute_reply":"2022-03-28T00:24:22.093734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_indices  = [int(te.split('\\n')[0]) for te in test_indices]","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:22.095540Z","iopub.execute_input":"2022-03-28T00:24:22.095955Z","iopub.status.idle":"2022-03-28T00:24:22.101859Z","shell.execute_reply.started":"2022-03-28T00:24:22.095919Z","shell.execute_reply":"2022-03-28T00:24:22.101220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(test_indices)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:22.103309Z","iopub.execute_input":"2022-03-28T00:24:22.103568Z","iopub.status.idle":"2022-03-28T00:24:22.109588Z","shell.execute_reply.started":"2022-03-28T00:24:22.103534Z","shell.execute_reply":"2022-03-28T00:24:22.108909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('../input/eclipse-preprocessed5new/eclipse_preprocessed_os.csv')\ndataset","metadata":{"execution":{"iopub.status.busy":"2022-03-28T17:31:11.707254Z","iopub.execute_input":"2022-03-28T17:31:11.707837Z","iopub.status.idle":"2022-03-28T17:31:16.583907Z","shell.execute_reply.started":"2022-03-28T17:31:11.707796Z","shell.execute_reply":"2022-03-28T17:31:16.583085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.loc[dataset['bug_id'] == 62303]['description1']","metadata":{"execution":{"iopub.status.busy":"2022-03-28T17:33:28.340289Z","iopub.execute_input":"2022-03-28T17:33:28.340579Z","iopub.status.idle":"2022-03-28T17:33:28.349379Z","shell.execute_reply.started":"2022-03-28T17:33:28.340547Z","shell.execute_reply":"2022-03-28T17:33:28.348536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.dropna(subset = [\"description1\"], inplace=True)\ndataset.dropna(subset = [\"description2\"], inplace=True)\ndataset.dropna(subset = [\"short_desc1\"], inplace=True)\ndataset.dropna(subset = [\"short_desc2\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:30.634878Z","iopub.execute_input":"2022-03-28T00:24:30.635228Z","iopub.status.idle":"2022-03-28T00:24:30.953530Z","shell.execute_reply.started":"2022-03-28T00:24:30.635189Z","shell.execute_reply":"2022-03-28T00:24:30.952755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset.loc[dataset['bug_id'] == 15957]","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:30.954919Z","iopub.execute_input":"2022-03-28T00:24:30.955197Z","iopub.status.idle":"2022-03-28T00:24:30.959807Z","shell.execute_reply.started":"2022-03-28T00:24:30.955161Z","shell.execute_reply":"2022-03-28T00:24:30.959146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in dataset.iterrows():\n    doc1 = 'Product:' + row['product'] + '.Component:' + row['component'] + '.Description:' + row['description1'] + '.Summary:' + row['short_desc1']\n#     print(row['bug_id'])\n#     print(row['description2'])\n    doc2 = 'Product:' + row['product'] + '.Component:' + row['component'] + '.Description:' + row['description2'] + '.Summary:' + row['short_desc2']\n    dataset.loc[index, 'doc1'] = doc1\n    dataset.loc[index, 'doc2'] = doc2","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:24:30.961038Z","iopub.execute_input":"2022-03-28T00:24:30.961453Z","iopub.status.idle":"2022-03-28T01:23:07.463473Z","shell.execute_reply.started":"2022-03-28T00:24:30.961416Z","shell.execute_reply":"2022-03-28T01:23:07.462593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in dataset.iterrows():\n    desc = row['doc1'] + row['doc2']\n    dataset.loc[index, 'description'] = desc","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:23:07.465406Z","iopub.execute_input":"2022-03-28T01:23:07.465825Z","iopub.status.idle":"2022-03-28T01:43:39.435512Z","shell.execute_reply.started":"2022-03-28T01:23:07.465760Z","shell.execute_reply":"2022-03-28T01:43:39.434771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reset_column_names():\n  dataset.drop('description', axis=1, inplace=True)\n#   dataset.drop('short_desc1', axis=1, inplace=True)\n#   dataset.drop('description2', axis=1, inplace=True)\n#   dataset.drop('short_desc2', axis=1, inplace=True)\n\n  dataset.rename(columns={'description_clean':'description'}, inplace=True) #,'short_desc1_clean':'short_desc1','description2_clean':'description2','short_desc2_clean':'short_desc2'","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:43:39.436903Z","iopub.execute_input":"2022-03-28T01:43:39.437166Z","iopub.status.idle":"2022-03-28T01:43:39.442842Z","shell.execute_reply.started":"2022-03-28T01:43:39.437132Z","shell.execute_reply":"2022-03-28T01:43:39.441788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text_content(text):\n    text = text.lower()\n#     cleaned_text = re.sub(r\"\"\"\n#                [,.;:@#?!&$*+-=_%<>\\/\\[\\]\\(\\)\\{\\}\\\"\\'\\n]+  # Accept one or more copies of punctuation\n#                \\ *           # plus zero or more copies of a space,\n#                \"\"\",\n#                \" \",          # and replace it with a single space\n#                text, flags=re.VERBOSE)\n#     cleaned_text = re.sub(r\"https?:\\/\\/[A-Za-z0-9.\\/?&#+*+-=_%]+\", \" \",text) #replace urls\n    cleaned_text = re.sub(r\" +\", \" \", text) #remove extra white spaces\n#     cleaned_text = re.sub(r\"[d|D]escription\", \"\", cleaned_text)\n    \n    return cleaned_text","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:43:39.444133Z","iopub.execute_input":"2022-03-28T01:43:39.444920Z","iopub.status.idle":"2022-03-28T01:43:39.457180Z","shell.execute_reply.started":"2022-03-28T01:43:39.444883Z","shell.execute_reply":"2022-03-28T01:43:39.456424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['description_clean']= dataset['description'].apply(lambda x:preprocess_text_content(x))\n# dataset['short_desc1_clean']= dataset['short_desc1'].apply(lambda x:preprocess_text(x))\n# dataset['description2_clean']= dataset['description2'].apply(lambda x:preprocess_text(x))\n# dataset['short_desc2_clean']= dataset['short_desc2'].apply(lambda x:preprocess_text(x))\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:43:39.458281Z","iopub.execute_input":"2022-03-28T01:43:39.458691Z","iopub.status.idle":"2022-03-28T01:43:56.363841Z","shell.execute_reply.started":"2022-03-28T01:43:39.458658Z","shell.execute_reply":"2022-03-28T01:43:56.363075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_column_names()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:43:56.365062Z","iopub.execute_input":"2022-03-28T01:43:56.366191Z","iopub.status.idle":"2022-03-28T01:43:56.601054Z","shell.execute_reply.started":"2022-03-28T01:43:56.366148Z","shell.execute_reply":"2022-03-28T01:43:56.600290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def preprocess_text(text,tokenizer):\n# #     print(text)\n#     #tokenize the text\n#     text = tokenizer.tokenize(text)\n#     #lowercase the text\n#     text = [w.lower() for w in text]\n#     #removal of stop word \n#     text = [w for w in text if w not in stop_words]\n#     #removal of non-english words\n#     text = [w for w in text if w in words or w.isalpha()]\n    #stemming\n#     text = [ ps.stem(w) for w in text]\n    #remove extra white spaces\n#     text = [ re.sub(r\" +\", \" \", w) for w in text] \n#     cleaned_text = re.sub(r\"\"\"\n#                [,.;:@#?!&$*+-=_%<>\\/\\[\\]\\(\\)\\{\\}\\\"\\'\\n]+  # Accept one or more copies of punctuation\n#                \\ *           # plus zero or more copies of a space,\n#                \"\"\",\n#                \" \",          # and replace it with a single space\n#                text, flags=re.VERBOSE)\n#     cleaned_text = re.sub(r\"https?:\\/\\/[A-Za-z0-9.\\/?&#+*+-=_%]+\", \" \",cleaned_text) #replace urls\n    \n#     cleaned_text = re.sub(r\"[d|D]escription\", \"\", cleaned_text)\n    \n#     return text","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:43:56.602554Z","iopub.execute_input":"2022-03-28T01:43:56.602818Z","iopub.status.idle":"2022-03-28T01:43:56.607273Z","shell.execute_reply.started":"2022-03-28T01:43:56.602782Z","shell.execute_reply":"2022-03-28T01:43:56.606526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0\n# lenArr = []\n# outliers = []\n\n# q25, q75 = np.percentile(lenArr, [25, 75])\n# bin_width = 2 * (q75 - q25) * len(lenArr) ** (-1/3)\n# bins = round((lenArr.max() - lenArr.min()) / bin_width)\n\n### Add tokens to the data make it BERT compatible\ndef bert_encode(texts, tokenizer, max_len=512):\n    print('encode')\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n#         text = preprocess_text(text,tokenizer)\n        text = tokenizer.tokenize(text)\n#         print(text)\n        text = [w for w in text if w.lower() not in stop_words]\n    \n#         length = len(text)\n#         if(length <1000):\n#             lenArr.append(length)\n#     #     print(length)\n#             if(max_len < length):\n#               max_len = length\n#         else:\n#             outliers.append(length)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n#     print(len(lenArr))\n#     # lenArr=list(filter(lambda a: a != 3395, lenArr))\n#     # print(lenArr)\n#     print(\"avg\", sum(lenArr)/len(lenArr))\n#     print(\"outliers\", outliers)\n#     print(\"outliers\", len(outliers))\n#     plt.hist(lenArr, density=True, bins=30)\n#     max_len\n\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:43:56.608760Z","iopub.execute_input":"2022-03-28T01:43:56.609305Z","iopub.status.idle":"2022-03-28T01:43:56.621500Z","shell.execute_reply.started":"2022-03-28T01:43:56.609263Z","shell.execute_reply":"2022-03-28T01:43:56.620787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :] \n   \n    clf_output = Reshape((1024,1))(clf_output)\n    print(clf_output)\n    \n    x = Conv1D(32, 3, activation='relu', padding='same')(clf_output)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D()(x)\n    x = Conv1D(32, 3, activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D()(x)\n    x = Flatten()(x)\n    out = Dense(1, activation='sigmoid')(x)\n#     out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:43:56.622918Z","iopub.execute_input":"2022-03-28T01:43:56.623520Z","iopub.status.idle":"2022-03-28T01:43:56.635575Z","shell.execute_reply.started":"2022-03-28T01:43:56.623480Z","shell.execute_reply":"2022-03-28T01:43:56.634801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(dataset, test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:43:56.640668Z","iopub.execute_input":"2022-03-28T01:43:56.641302Z","iopub.status.idle":"2022-03-28T01:43:56.815823Z","shell.execute_reply.started":"2022-03-28T01:43:56.641273Z","shell.execute_reply":"2022-03-28T01:43:56.815063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FullTokenizer = tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\", trainable=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:43:56.817146Z","iopub.execute_input":"2022-03-28T01:43:56.817494Z","iopub.status.idle":"2022-03-28T01:44:32.863575Z","shell.execute_reply.started":"2022-03-28T01:43:56.817454Z","shell.execute_reply":"2022-03-28T01:44:32.862848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nlower_cased = bert_layer.resolved_object.do_lower_case.numpy()\n\ntokenizer = FullTokenizer(vocab_file, lower_cased)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:44:32.864944Z","iopub.execute_input":"2022-03-28T01:44:32.865353Z","iopub.status.idle":"2022-03-28T01:44:32.976353Z","shell.execute_reply.started":"2022-03-28T01:44:32.865316Z","shell.execute_reply":"2022-03-28T01:44:32.975683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X = dataset['description']\n# Y = np.array(dataset.duplicate.values, dtype='int')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:44:32.977494Z","iopub.execute_input":"2022-03-28T01:44:32.977777Z","iopub.status.idle":"2022-03-28T01:44:32.981811Z","shell.execute_reply.started":"2022-03-28T01:44:32.977741Z","shell.execute_reply":"2022-03-28T01:44:32.981127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_input = bert_encode(train['description'], tokenizer, max_len=160)\n# train_labels = np.array(train.duplicate.values, dtype='int')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:44:32.983021Z","iopub.execute_input":"2022-03-28T01:44:32.983419Z","iopub.status.idle":"2022-03-28T01:44:32.989929Z","shell.execute_reply.started":"2022-03-28T01:44:32.983383Z","shell.execute_reply":"2022-03-28T01:44:32.989262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model(\"../input/eclipse-netbeans-combined-model/eclipse_netbeans_bert_cnn_one_iteration\")\n# model.summary()\n\n# Fit the model\n# model.fit(train_input,train_labels, validation_split=0.4, epochs=2, batch_size=10)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:44:32.991190Z","iopub.execute_input":"2022-03-28T01:44:32.991441Z","iopub.status.idle":"2022-03-28T01:45:29.832725Z","shell.execute_reply.started":"2022-03-28T01:44:32.991407Z","shell.execute_reply":"2022-03-28T01:45:29.831962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save(\"eclipse_bert_cnn\")","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:45:29.834205Z","iopub.execute_input":"2022-03-28T01:45:29.834446Z","iopub.status.idle":"2022-03-28T01:45:29.838607Z","shell.execute_reply.started":"2022-03-28T01:45:29.834413Z","shell.execute_reply":"2022-03-28T01:45:29.837910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input = bert_encode(test['description'], tokenizer, max_len=160)\ntest_labels = np.array(test.duplicate.values, dtype='int')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:45:29.840228Z","iopub.execute_input":"2022-03-28T01:45:29.840788Z","iopub.status.idle":"2022-03-28T01:57:56.767723Z","shell.execute_reply.started":"2022-03-28T01:45:29.840751Z","shell.execute_reply":"2022-03-28T01:57:56.766909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.evaluate(test_input, test_labels, verbose=0)\nf1score = (2 * scores[2]*100 * scores[3]*100)/(scores[2]*100 + scores[3]*100)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\nprint(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\nprint(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\nprint(\"f1 score: \", (f1score))","metadata":{"execution":{"iopub.status.busy":"2022-03-28T01:57:56.769145Z","iopub.execute_input":"2022-03-28T01:57:56.769682Z","iopub.status.idle":"2022-03-28T02:21:22.679418Z","shell.execute_reply.started":"2022-03-28T01:57:56.769642Z","shell.execute_reply":"2022-03-28T02:21:22.678675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_input = bert_encode(dataset['description'], tokenizer, max_len=160)\n# test_input = bert_encode(test['description'], tokenizer, max_len=160)\n# train_labels = np.array(train.duplicate.values, dtype='int')\n# test_labels = np.array(test.duplicate.values, dtype='int')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T02:21:22.680857Z","iopub.execute_input":"2022-03-28T02:21:22.681129Z","iopub.status.idle":"2022-03-28T02:21:22.685021Z","shell.execute_reply.started":"2022-03-28T02:21:22.681087Z","shell.execute_reply":"2022-03-28T02:21:22.684065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_len = 0\n# lenArr = []\n# outliers = []\n\n# for index, row in dataset.iterrows():\n#     # print(train.loc[[index]]['description'])\n#     length = max([len(s.split()) for s in dataset.loc[[index]]['description']])\n#     if(length <1000):\n#         lenArr.append(length)\n# #     print(length)\n#         if(max_len < length):\n#           max_len = length\n#     else:\n#         outliers.append(length)\n\n# # q25, q75 = np.percentile(lenArr, [25, 75])\n# # bin_width = 2 * (q75 - q25) * len(lenArr) ** (-1/3)\n# # bins = round((lenArr.max() - lenArr.min()) / bin_width)\n# print(len(lenArr))\n# # lenArr=list(filter(lambda a: a != 3395, lenArr))\n# # print(lenArr)\n# print(\"avg\", sum(lenArr)/len(lenArr))\n# print(\"outliers\", outliers)\n# print(\"outliers\", len(outliers))\n# plt.hist(lenArr, density=True, bins=30)\n# max_len","metadata":{"execution":{"iopub.status.busy":"2022-03-28T02:21:22.686386Z","iopub.execute_input":"2022-03-28T02:21:22.686635Z","iopub.status.idle":"2022-03-28T02:21:22.695684Z","shell.execute_reply.started":"2022-03-28T02:21:22.686600Z","shell.execute_reply":"2022-03-28T02:21:22.694791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = build_model(bert_layer, max_len=160)\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T02:21:22.697993Z","iopub.execute_input":"2022-03-28T02:21:22.698262Z","iopub.status.idle":"2022-03-28T02:21:22.704275Z","shell.execute_reply.started":"2022-03-28T02:21:22.698228Z","shell.execute_reply":"2022-03-28T02:21:22.703519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_history = model.fit(\n#     train_input, train_labels,\n#     validation_split=0.3,\n#     epochs=5,\n#     batch_size=50\n# )","metadata":{"execution":{"iopub.status.busy":"2022-03-28T02:21:22.705594Z","iopub.execute_input":"2022-03-28T02:21:22.705842Z","iopub.status.idle":"2022-03-28T02:21:22.712534Z","shell.execute_reply.started":"2022-03-28T02:21:22.705810Z","shell.execute_reply":"2022-03-28T02:21:22.711797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluation_results = model.evaluate(test_input,test_labels,return_dict=True)\n# evaluation_results","metadata":{"execution":{"iopub.status.busy":"2022-03-28T02:21:22.713850Z","iopub.execute_input":"2022-03-28T02:21:22.715057Z","iopub.status.idle":"2022-03-28T02:21:22.720472Z","shell.execute_reply.started":"2022-03-28T02:21:22.714989Z","shell.execute_reply":"2022-03-28T02:21:22.719467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X = dataset['description']\n# Y = np.array(dataset.duplicate.values, dtype='int')\n\n# kfold = StratifiedKFold()\n# cvscores = []\n# precision_scores = []\n# recall_scores = []\n# f1_scores = []\n    \n# for train, test in kfold.split(X, Y):\n#     # Prepare data\n#     train_input = bert_encode(X.iloc[train], tokenizer, max_len=160)\n#     test_input = bert_encode(X.iloc[test], tokenizer, max_len=160)\n#     train_labels = Y[train]\n#     test_labels = Y[test]\n    \n#     model = build_model(bert_layer, max_len=160)\n#     model.summary()\n\n#     # Fit the model\n#     model.fit(train_input,train_labels, epochs=2, batch_size=15)\n\n#     # Evaluate the model\n#     scores = model.evaluate(test_input, test_labels, verbose=0)\n#     f1score = (2 * scores[2]*100 * scores[3]*100)/(scores[2]*100 + scores[3]*100)\n#     print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n#     print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n#     print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n#     print(\"f1 score: \", (f1score))\n#     cvscores.append(scores[1] * 100)\n#     precision_scores.append(scores[2] * 100)\n#     recall_scores.append(scores[3] * 100)\n#     f1_scores.append(f1score)\n    \n\n# print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n# print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision_scores), np.std(precision_scores)))\n# print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall_scores), np.std(recall_scores)))\n# print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))","metadata":{"_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2022-03-28T02:21:22.721654Z","iopub.execute_input":"2022-03-28T02:21:22.722368Z","iopub.status.idle":"2022-03-28T02:21:22.728862Z","shell.execute_reply.started":"2022-03-28T02:21:22.722331Z","shell.execute_reply":"2022-03-28T02:21:22.728170Z"},"trusted":true},"execution_count":null,"outputs":[]}]}